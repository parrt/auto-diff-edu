\documentclass[11pt]{article}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{epstopdf}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

\textwidth = 6.5 in
\textheight = 9 in
\oddsidemargin = 0.0 in
\evensidemargin = 0.0 in
\topmargin = 0.0 in
\headheight = 0.0 in
\headsep = 0.0 in
\parskip = 0.2in
\parindent = 0.0in

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}{Definition}

\title{Brief Article}
\author{The Author}
\begin{document}
\maketitle

\section{Vector calculus}

Assume all vectors are vertical by default of size $n \times 1$. Lowercase letters in bold font such as $\mathbf{x}$ are vectors and math italics font like $x$ or scalars so:

$\mathbf{x} = \begin{bmatrix}
           x_1\\
           x_2\\
           \vdots \\
           x_n\\
           \end{bmatrix}$

Let $\mathbf{y} = \mathbf{f}(\mathbf{x})$ be a vector of $m$ scalar-valued functions, $f_i : R^n \rightarrow R$ with $n=|\mathbf{x}|$. $\mathbf{y} = \mathbf{f}(\mathbf{x})$ is a set of $m$ equations that are functions of $\mathbf{x}$:

$
\begin{array}{lcl}
y_1 & = & f_1(\mathbf{x})\\
y_2 & = & f_2(\mathbf{x})\\
 & \vdots & \\
y_m & = & f_m(\mathbf{x})\\
\end{array}
$

The Jacobian matrix is the collection of all $m \times n$ possible partial derivatives:

$
\frac{\partial \mathbf{y}}{\partial \mathbf{x}} = \begin{bmatrix}
\frac{\partial}{\partial {x}} f_1(\mathbf{x}) \\
\frac{\partial}{\partial {x}} f_2(\mathbf{x})\\
\ldots\\
\frac{\partial}{\partial {x}} f_m(\mathbf{x})
\end{bmatrix}
$

where each $\frac{\partial}{\partial {x}} f_i(\mathbf{x})$ is an $n$-vector because the partial derivative is with respect to a vector, $\mathbf{x}$, whose length is $n = |\mathbf{x}|$.

The width of the Jacobian is the length of the vector $\mathbf{x}$ if we're taking partial derivative with respect to $\mathbf{x}$.  The Jacobian is $m$ rows for $m$ equations.

To represent the trivial equation $\mathbf{y} = \mathbf{x}$, for example, $f_i(\mathbf{x}) = x_i$. In this case, the Jacobian is the identity matrix since $m=n$ (a Jacobian matrix size $m \times n$ where $m = |\mathbf{f}| = n = |\mathbf{x}|$).

$
\frac{\partial \mathbf{y}}{\partial \mathbf{x}} = \begin{bmatrix}
\frac{\partial}{\partial {x}} f_1(\mathbf{x}) \\
\frac{\partial}{\partial {x}} f_2(\mathbf{x})\\
\ldots\\
\frac{\partial}{\partial {x}} f_m(\mathbf{x})
\end{bmatrix} = \begin{bmatrix}
\frac{\partial}{\partial {x_1}} f_1(\mathbf{x}) \frac{\partial}{\partial {x_2}} f_1(\mathbf{x}) \ldots \frac{\partial}{\partial {x_n}} f_1(\mathbf{x}) \\
\frac{\partial}{\partial {x_1}} f_2(\mathbf{x}) \frac{\partial}{\partial {x_2}} f_2(\mathbf{x}) \ldots \frac{\partial}{\partial {x_n}} f_2(\mathbf{x}) \\
\ldots\\
\frac{\partial}{\partial {x_1}} f_m(\mathbf{x}) \frac{\partial}{\partial {x_2}} f_m(\mathbf{x}) \ldots \frac{\partial}{\partial {x_n}} f_m(\mathbf{x}) \\
\end{bmatrix} = \begin{bmatrix}
\frac{\partial}{\partial {x_1}} x_1 \frac{\partial}{\partial {x_2}} x_1 \ldots \frac{\partial}{\partial {x_n}} x_1 \\
\frac{\partial}{\partial {x_1}} x_2 \frac{\partial}{\partial {x_2}} x_2 \ldots \frac{\partial}{\partial {x_n}} x_2 \\
\ldots\\
\frac{\partial}{\partial {x_1}} x_m \frac{\partial}{\partial {x_2}} x_m \ldots \frac{\partial}{\partial {x_n}} x_m \\
\end{bmatrix} = \begin{bmatrix}
           \mathbf{i}_1\\
           \mathbf{i}_2\\
           \vdots \\
           \mathbf{i}_m\\
           \end{bmatrix} = I
$

where $\mathbf{i}_j$ is a vector with $\mathbf{i}_j=1$ and $\mathbf{i}_k=0$ for $k \neq j$ (vector with all zeros except at $j$, which is 1).

I found it very important to track whether a matrix is vertical or horizontal, $\mathbf{x}$ or $\mathbf{x}^T$. Also make sure you pay attention to whether something is a scalar-valued function or a vector valued function. Is it $\mathbf{y} = \ldots$ or $y = \ldots$?

The other trick is to reduce everything down to a set of scalar equations and then take all of the partials derivatives. This dictates whether you get a scalar, vector, or matrix Jacobian.

\subsection{Derivatives of vector-scalar multiplication}

$\mathbf{y} = \mathbf{x} \times c = \begin{bmatrix}
           x_{1} c\\
           x_{2} c\\
           \vdots \\
           x_{n} c
         \end{bmatrix}$

Just looking at vector-scalar multiplication, leads us to think that the shape of the Jacobian is a vertical vector containing all $c$ values but it's actually a matrix with $c$ down the diagonal, $I c$. We have a set of functions and a set of parameters, which yields a cross-product sized matrix as the Jacobian.

Let $y_i = f_i(x,c) = x_i c$ so $\frac{\partial y_i}{\partial {\mathbf{x}}} = [ \frac{\partial y_i}{\partial x_1} \frac{\partial y_i}{\partial x_2} \ldots \frac{\partial y_i}{\partial x_n} ]$ but there are $m=n$ $y_i$ equations,  which gives us the matrix:

$\frac{\partial \mathbf{y}}{\partial \mathbf{x}} =  \begin{bmatrix}
\frac{\partial}{\partial {x_1}} x_1 c~ \frac{\partial}{\partial {x_2}} x_1 c~ \ldots \frac{\partial}{\partial {x_n}} x_1 c \\
\frac{\partial}{\partial {x_1}} x_2 c~\frac{\partial}{\partial {x_2}} x_2 c~ \ldots \frac{\partial}{\partial {x_n}} x_2 c \\
\ldots\\
\frac{\partial}{\partial {x_1}} x_m c~ \frac{\partial}{\partial {x_2}} x_m c~ \ldots \frac{\partial}{\partial {x_n}} x_m c\\
\end{bmatrix} = \begin{bmatrix}
           \mathbf{i}_1 c\\
           \mathbf{i}_2 c\\
           \vdots \\
           \mathbf{i}_m c\\
           \end{bmatrix} = I \times c$

We could also use the product rule for $\mathbf{x} \times c$ to get Jacobian $I \times c$.
 
The derivative with respect to $c$ on the other hand, is a column vector:

$\frac{\partial}{\partial c} \mathbf{y} = \frac{\partial \mathbf{y}}{\partial c} = \begin{bmatrix}
           \frac{\partial}{\partial c} x_{1} c \\
           \frac{\partial}{\partial c} x_{2} c \\
           \vdots \\
           \frac{\partial}{\partial c} x_{m} c \\
         \end{bmatrix} = \begin{bmatrix}
           x_{1} \\
           x_{2} \\
           \vdots \\
           x_{m} 
         \end{bmatrix} = \mathbf{x}$

(Again, $m=n$ here.)

Note: the Jacobian is an $n \times 1$ matrix (vertical vector) not horizontal gradient vector because it's like we have $n$ simple expressions $y_i = x_i c$.

\subsection{Derivatives of vector-scalar addition}

To add a scalar to a vector, we need to expand the scalar.  Equation $\mathbf{y} = f(\mathbf{x},c) = \mathbf{x} + c$ expands as: $\mathbf{y} = \mathbf{x} + \vec{1} \times c$.  From above, we can right the Jacobian down as

$
\frac{\partial y}{\partial {\mathbf{x}}} = \frac{\partial \mathbf{x}}{\partial \mathbf{x}} + \frac{\partial}{\partial \mathbf{x}}(\vec{1}\times c) =  \frac{\partial \mathbf{x}}{\partial \mathbf{x}} + \mathbf{0}_n = I.
$

where $\mathbf{0}_{n}$ is the $n \times n$ zero matrix. Similarly, using the vector-scalar multiplication from above, we get:

$\frac{\partial y}{\partial c} = \frac{\partial \mathbf{x}}{\partial c} + \frac{\partial}{\partial c} (\vec{1} \times c) = \vec{0} + \vec{1} = \vec{1}$.

\subsection{Derivatives of vector-vector addition}

We've already done a vector addition in the previous section, but it's worth spelling out:
 
$y = \mathbf{w} + \mathbf{x}$

We get $\frac{\partial \mathbf{y}}{\partial \mathbf{w}} = \frac{\partial \mathbf{w}}{\partial \mathbf{w}} + \frac{\partial \mathbf{x}}{\partial \mathbf{w}} = I + \mathbf{0}_n = I$. Similarly, $\frac{\partial \mathbf{y}}{\partial \mathbf{x}} = I$.

\subsection{Derivatives of vector dot product}

For vector dot product $y = \mathbf{f(x)} \cdot \mathbf{g(x)}$ (note $y$ not $\mathbf{y}$ as the result of that product is a scalar), we get:

$
\begin{array}{rcl}
\frac{\partial y}{\partial \mathbf{x}} & = & \mathbf{f(x)} \cdot \frac{\partial \mathbf{g(x)}}{\partial \mathbf{x}} + \mathbf{g(x)} \cdot \frac{\partial \mathbf{f(x)}}{\partial \mathbf{x}}\\\\
 & = & \mathbf{f(x)} \cdot \begin{bmatrix} \frac{\partial g(\mathbf{x})}{\partial x_1},~ \frac{\partial g(\mathbf{x})}{\partial x_2},~ \ldots,~ \frac{\partial g(\mathbf{x})}{\partial x_n} \end{bmatrix}  + \mathbf{g(x)} \cdot \begin{bmatrix} \frac{\partial f(\mathbf{x})}{\partial x_1},~ \frac{\partial f(\mathbf{x})}{\partial x_2},~ \ldots,~ \frac{\partial f(\mathbf{x})}{\partial x_n} \end{bmatrix}\\

\end{array}
$

Note that the partial derivatives are $\frac{\partial f(\mathbf{x})}{\partial x_i}$ not $\frac{\partial f(x_i)}{\partial x_i}$ and not $\frac{\partial f_i(x_i)}{\partial x_i}$.

As an example, consider $y = \mathbf{w} \cdot \mathbf{x}$. The partial derivative with respect to either factor is a gradient, horizontal vector:

$\frac{\partial y}{\partial \mathbf{w}} = \mathbf{w} \cdot \frac{\partial \mathbf{x}}{\partial \mathbf{w}} + \mathbf{x} \cdot \frac{\partial \mathbf{w}}{\partial \mathbf{w}} = \mathbf{w}^T \times \frac{\partial \mathbf{x}}{\partial \mathbf{w}} + \mathbf{x}^T \times \frac{\partial \mathbf{w}}{\partial \mathbf{w}} = \mathbf{w}^T \times \mathbf{0} + \mathbf{x}^T \times I = \mathbf{x}^T$. Similarly, $\frac{\partial y}{\partial \mathbf{x}} = \mathbf{w}^T$.

Note: the dot product transforms into vector-vector multiplication via:

$y = \mathbf{w} \cdot \mathbf{x} = \mathbf{w}^{T} \times \mathbf{x}$

Note: the gradient is a horizontal vector (1 output)!

We can also grind the dot product down into a pure scalar function:

$y = \mathbf{w} \cdot \mathbf{x} = \Sigma_i^n (w_i x_i)$

$\frac{\partial y}{\partial w_j} = \frac{\partial}{\partial w_j} \Sigma_i (w_i x_i) = \Sigma_i \frac{\partial}{\partial w_j} (w_i x_i) = \frac{\partial}{\partial w_j} (w_j x_j) = x_j$

Then:

$\frac{\partial y}{\partial \mathbf{w}} = [ x_1, \ldots, x_n ] = \mathbf{x}^T$

\subsection{Vector sum}

Let $y = sum( \mathbf{f}(\mathbf{x})) = \Sigma_{i=1}^n f_i(\mathbf{x})$ then

$
\begin{array}{lcl}
\frac{\partial y}{\partial \mathbf{x}} & = & \begin{bmatrix} \frac{\partial y}{\partial x_1}, \frac{\partial y}{\partial x_2}, \ldots, \frac{\partial y}{\partial x_n} \end{bmatrix}\\\\
 & = & \begin{bmatrix} \frac{\partial}{\partial x_1} \Sigma_i f_i(\mathbf{x}),~ \frac{\partial}{\partial x_2} \Sigma_i f_i(\mathbf{x}),~ \ldots,~ \frac{\partial}{\partial x_n} \Sigma_i  f_i(\mathbf{x}) \end{bmatrix} \\\\
 & = & \begin{bmatrix} \Sigma_i \frac{\partial f_i(\mathbf{x})}{\partial x_1},~ \Sigma_i \frac{\partial f_i(\mathbf{x})}{\partial x_2},~ \ldots,~ \Sigma_i \frac{\partial f_i(\mathbf{x})}{\partial x_n}  \end{bmatrix}(\text{move derivative inside})\\\\
\end{array}
$

Example: Sum of vector-scalar multiplication.


If $y = sum(\mathbf{x} \times c)$ then $f_i(\mathbf{x},c) = x_i \times c$.

$
\begin{array}{lcl}
\frac{\partial y}{\partial \mathbf{x}} & = & \begin{bmatrix} \Sigma_i \frac{\partial}{\partial x_1} x_i c,~ \Sigma_i \frac{\partial }{\partial x_2} x_i c,~ \ldots,~ \Sigma_i \frac{\partial}{\partial x_n} x_i c  \end{bmatrix}\\\\
 & = & \begin{bmatrix} \frac{\partial}{\partial x_1} x_1 c,~ \frac{\partial }{\partial x_2} x_2 c,~ \ldots,~ \frac{\partial}{\partial x_n} x_n c  \end{bmatrix}\\\\
 & = & \begin{bmatrix} c, c, \ldots, c \end{bmatrix}\\\\
\end{array}
$

The derivative with respect to scalar variable $c$ is $1 \times 1$:

$
\begin{array}{lcl}
\frac{\partial y}{\partial c} & = & \frac{\partial}{\partial c} \Sigma_{i=1}^n (x_i+c)\\
& = & \Sigma_i \frac{\partial}{\partial c} (x_i+c)\\
& = & \Sigma_i (0 + 1)\\
& = & n
\end{array}
$

\section{Acknowledgements}

We thank [Yannet Interian](https://www.usfca.edu/faculty/yannet-interian) (Faculty in MS data science program at University of San Francisco) and [David Uminsky](http://www.cs.usfca.edu/~duminsky/) (Faculty/director of MS data science) for their help with the notation presented here..

\end{document}
