\documentclass[11pt]{article}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{epstopdf}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

\textwidth = 6.5 in
\textheight = 9 in
\oddsidemargin = 0.0 in
\evensidemargin = 0.0 in
\topmargin = 0.0 in
\headheight = 0.0 in
\headsep = 0.0 in
\parskip = 0.2in
\parindent = 0.0in

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}{Definition}

\newcommand{\cut}[1]{}

\title{Matrix Calculus Jumpstart}
\author{Terence Parr and Jeremy Howard}
\begin{document}
\maketitle

\section{Introduction}

\cut{
three-dimensional coordinates in room yielding a temperature. then can yield temperature and noise or humidity or light.  Sheldon Cooper: you're in my spot.

$\Delta x$ gives what $\Delta y$? rise/run to determine derivative.

neural net equation. optimizing it and calculus comes lurching back into your life like distant relatives around the holidays.

 lots of good cheat sheets like https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf and slides like https://www.comp.nus.edu.sg/~cs5240/lecture/matrix-differentiation.pdf
 
given any particular formula, you can reduce it to its scalar form and use the typical scalar derivative to come up with the formula. That can be tedious and error-prone. It's better to learn the matrix/vector derivatives and apply transformations to get the derivative.
}

\section{Review: Scalar derivative rules}

$
\begin{array}{lcc}
Rule & \text{Scalar function } f(x) & \text{Scalar derivative with repsect to } x\\
\hline
\text{Multiplication by constant} &	cf(x)	&cf'(x)\\
\text{Power Rule}	& x^n	& nx^{n-1}\\
\text{Sum Rule}	& f + g	& f' + g'\\
\text{Difference Rule}	& f - g	& f' - g'\\
\text{Product Rule}	& fg & f g' + f' g\\
\text{Quotient Rule}	& \frac{f}{g} = fg^{-1} & f (g^{-1})' + f' g^{-1}\\
\text{Chain Rule}	 & f(g(x)) &   f'(g(x))g'(x)\\
\end{array}
$

When a function has a single parameter, $f(x)$, you'll often see $f'$ and $f'(x)$ used as shorthands for $\frac{d}{dx} f(x)$.  

You can think of $\frac{d}{dx}$ as an operator that maps a function of one parameter to another function. So to take the derivative with respect to $x$ of $cf(x)$, $\frac{d}{dx} cf(x)$, we are really just moving the constant to the left of the operator: $c\frac{d}{dx}x$ or $c\frac{dx}{dx}$, which makes it more clear that the derivative is $c \times 1 = c$.  We are used to this simplification for the usual arithmetic operators. For example, $-2x = 2 \times -x$ where the negation operator is analogous to our derivative operator. We have just moved to the scalar outside of the operation. Thinking of the derivative as an operator is handy when solving complicated derivatives as we can mechanically apply rules of differentiation.

\section{Review: Vector calculus and partial derivatives}

Real-world problems are rarely so simple that we can express them as a single function of a single parameter, $f(x)$. Let's move on to functions of multiple parameters such as $f(x,y)$. We compute derivatives with respect to one variable (parameter) at a time, giving us two different {\em partial derivatives} for this two-parameter function.  Instead of using operator $\frac{d}{dx}$, we use $\frac{\partial}{\partial x}$ (a stylized $d$ and not the Greek letter $\delta$). For functions of a single parameter, operator $\frac{\partial}{\partial x}$ is equivalent to $\frac{d}{dx}$ but you should  restrict your use of $\frac{d}{dx}$ to functions of a single parameter.

The partial derivative with respect to $x$ is just the usual scalar derivative but treating any other variable in the equation as a constant.  Consider function $f(x,y) = 3x^2y$. The partial derivative with respect to $x$ is written $\frac{\partial}{\partial x} 3x^2y$. There are three constants from the perspective of $\frac{\partial}{\partial x}$: 3, 2, and $y$. Therefore, $\frac{\partial}{\partial x} 3yx^2 = 3y\frac{\partial}{\partial x} x^2 = 3y2x = 6yx$. The partial derivative with respect to $y$ treats $x$ like a constant now: $\frac{\partial}{\partial y} 3x^2y = 3x^2\frac{\partial}{\partial y} y = 3x^2\frac{\partial y}{\partial y} = 3x^2 \times 1 = 3x^2$. 

So, partial derivatives use the usual scalar derivative rules while holding all but a single variable, $x$, as constants for the purpose of computing the partial derivative with respect to $x$.

To make it clear we are doing vector calculus and not just multivariate calculus, let's consider what we do with the partial derivatives $\frac{\partial f(x,y)}{\partial x}$ and $\frac{\partial f(x,y)}{\partial y}$ we computed for $f(x,y) = 3x^2y$. Currently they're just kind of floating around and not organized in any way so let's organize them into a horizontal vector:

$[ \frac{\partial f(x,y)}{\partial x}, \frac{\partial f(x,y)}{\partial y}] = [6yx, 3x^2]$

We call this vector the {\em gradient} of $f(x,y)$ and write it as:

$\nabla f(x,y) = [6yx, 3x^2]$.

Vector calculus deals with functions mapping $n$ real parameters to a real scalar, $f : \mathbb{R}^n \rightarrow \mathbb{R}$.  Now, let's get crazy and consider multiple functions simultaneously, $\mathbf{f}: \mathbb{R}^n \rightarrow \mathbb{R}^m$.

\section{Matrix calculus}

Moving from vector calculus to matrix calculus now, let's compute partial derivatives for two functions, both of which take two parameters.  We can keep the same $f(x,y)$ from the last section but let's add $g(x,y) = 2x + y^8$.  The gradient for $g$ has two entries, a partial derivative for each parameter. The partials are:

$\frac{\partial g(x,y)}{\partial x} = \frac{\partial 2x}{\partial x} + \frac{\partial y^8}{\partial x} = 2\frac{\partial x}{\partial x} + 0 = 2 \times 1 = 1$

and

$\frac{\partial g(x,y)}{\partial y} = \frac{\partial 2x}{\partial y} + \frac{\partial y^8}{\partial y} = 0 + 8y^7 = 8y^7$

giving us gradient $\nabla g(x,y) = [1, 8y^7]$.

Gradient vectors organize all of the partial derivatives for a specific scalar function. If we have two functions, we can also organize their gradients into a matrix called the {\em Jacobian matrix} where the gradients are rows:

$J =
\begin{bmatrix}
	\nabla f(x,y)\\
	\nabla g(x,y)
\end{bmatrix} = \begin{bmatrix}
	6yx & 3x^2\\
	1 & 8y^7
\end{bmatrix}
$

Welcome to matrix calculus!

\includegraphics[scale=.08]{redbang.png} Before moving on, please note that there are multiple ways to represent the Jacobian. We are using the so-called [numerator layout](https://en.wikipedia.org/wiki/Matrix\_calculus\#Layout\_conventions) but many papers and software will use the transpose of this Jacobian (flip it around its diagonal).

\subsection{Generalization of the Jacobian}

So far, we've looked at a specific example of a Jacobian matrix. To define a Jacobian matrix, let's combine multiple parameters into a single vector argument: $f(x,y,z) \Rightarrow f(\mathbf{x})$. Lowercase letters in bold font such as $\mathbf{x}$ are vectors and math italics font like $x$ are scalars; $x_i$ is the $i{th}$ element of vector $\mathbf{x}$. We also have to define an orientation for vector $\mathbf{x}$. We'll assume that all vectors are vertical by default of size $n \times 1$:

$\mathbf{x} = \begin{bmatrix}
           x_1\\
           x_2\\
           \vdots \\
           x_n\\
           \end{bmatrix}$

With multiple scalar-valued functions, we can combine them all into a vector just like we did with the parameters. Let $\mathbf{y} = \mathbf{f}(\mathbf{x})$ be a vector of $m$ scalar-valued functions, $f_i : R^n \rightarrow R$ with $n=|\mathbf{x}|$. $\mathbf{y} = \mathbf{f}(\mathbf{x})$ is a set of $m$ equations that are functions of vector $\mathbf{x}$:

$
\begin{array}{lcl}
y_1 & = & f_1(\mathbf{x})\\
y_2 & = & f_2(\mathbf{x})\\
 & \vdots & \\
y_m & = & f_m(\mathbf{x})\\
\end{array}
$

It's very often the case that $m=n$ because we will have a scalar function result for each element of the vector; this is the case for $\mathbf{y} = \mathbf{f}(\mathbf{x})$.

The Jacobian matrix is the collection of all $m \times n$ possible partial derivatives:

$
\frac{\partial \mathbf{y}}{\partial \mathbf{x}} = \begin{bmatrix}
\frac{\partial}{\partial \mathbf{x}} f_1(\mathbf{x}) \\
\frac{\partial}{\partial \mathbf{x}} f_2(\mathbf{x})\\
\ldots\\
\frac{\partial}{\partial \mathbf{x}} f_m(\mathbf{x})
\end{bmatrix}
$

where each $\frac{\partial}{\partial \mathbf{x}} f_i(\mathbf{x})$ is an $n$-vector because the partial derivative is with respect to a vector, $\mathbf{x}$, whose length is $n = |\mathbf{x}|$.

The width of the Jacobian is the length of the vector $\mathbf{x}$ if we're taking partial derivative with respect to $\mathbf{x}$.  The Jacobian is always $m$ rows for $n$ equations.

todo: Show the shape with boxes.  

To represent the trivial equation $\mathbf{y} = \mathbf{x}$, for example, $f_i(\mathbf{x}) = x_i$. In this case, the Jacobian is the identity matrix since $m=n$ (a Jacobian matrix size $m \times n$ where $m = |\mathbf{f}| = n = |\mathbf{x}|$).

$
\frac{\partial \mathbf{y}}{\partial \mathbf{x}} = \begin{bmatrix}
\frac{\partial}{\partial {x}} f_1(\mathbf{x}) \\
\frac{\partial}{\partial {x}} f_2(\mathbf{x})\\
\ldots\\
\frac{\partial}{\partial {x}} f_m(\mathbf{x})
\end{bmatrix} = \begin{bmatrix}
\frac{\partial}{\partial {x_1}} f_1(\mathbf{x}) \frac{\partial}{\partial {x_2}} f_1(\mathbf{x}) \ldots \frac{\partial}{\partial {x_n}} f_1(\mathbf{x}) \\
\frac{\partial}{\partial {x_1}} f_2(\mathbf{x}) \frac{\partial}{\partial {x_2}} f_2(\mathbf{x}) \ldots \frac{\partial}{\partial {x_n}} f_2(\mathbf{x}) \\
\ldots\\
\frac{\partial}{\partial {x_1}} f_m(\mathbf{x}) \frac{\partial}{\partial {x_2}} f_m(\mathbf{x}) \ldots \frac{\partial}{\partial {x_n}} f_m(\mathbf{x}) \\
\end{bmatrix} = \begin{bmatrix}
\frac{\partial}{\partial {x_1}} x_1 \frac{\partial}{\partial {x_2}} x_1 \ldots \frac{\partial}{\partial {x_n}} x_1 \\
\frac{\partial}{\partial {x_1}} x_2 \frac{\partial}{\partial {x_2}} x_2 \ldots \frac{\partial}{\partial {x_n}} x_2 \\
\ldots\\
\frac{\partial}{\partial {x_1}} x_m \frac{\partial}{\partial {x_2}} x_m \ldots \frac{\partial}{\partial {x_n}} x_m \\
\end{bmatrix} = \begin{bmatrix}
           \mathbf{i}_1\\
           \mathbf{i}_2\\
           \vdots \\
           \mathbf{i}_m\\
           \end{bmatrix} = I
$

where $\mathbf{i}_j$ is a vector with $\mathbf{i}_j=1$ and $\mathbf{i}_k=0$ for $k \neq j$ (vector with all zeros except at $j$, which is 1).

It's very important to track whether a matrix is vertical or horizontal, $\mathbf{x}$ or $\mathbf{x}^T$. Also make sure you pay attention to whether something is a scalar-valued function or a vector valued function. Is it $\mathbf{y} = \ldots$ or $y = \ldots$?

When trying to deduce the Jacobian of a complex vector equation, a good trick is to reduce everything down to a set of scalar equations and then take all of the partials derivatives, combining the results appropriately into vectors and matrices.

\subsection{Derivatives of vector element-wise binary operators}

Element-wise binary operations on vectors, such as vector addition $\mathbf{w} + \mathbf{x}$, are important because we can express many common vector operations, such as vector-scalar multiplication, as element-wise binary operations.  We can generalize the element-wise binary operations with notation $\mathbf{y} = \mathbf{f(x)} \bigcirc \mathbf{g(x)}$, where we're using $\bigcirc$ to represent any element-wise operator and not the $\circ$ function composition operator.  Here's what that equation looks like when we zoom in to examine the scalar equations:

$\begin{bmatrix}
           y_1\\
           y_2\\
           \vdots \\
           y_n\\
           \end{bmatrix} = \begin{bmatrix}
           f_{1}(\mathbf{x}) \bigcirc g_{1}(\mathbf{x})\\
           f_{n}(\mathbf{x}) \bigcirc g_{2}(\mathbf{x})\\
           \vdots \\
           f_{n}(\mathbf{x}) \bigcirc g_{n}(\mathbf{x})\\
         \end{bmatrix}$

where we write $n$ not $m$ equations vertically to emphasize the fact that the result of element-wise operators give $m=n$ sized vector results.

The general case for the Jacobian is the square matrix:

$
\frac{\partial \mathbf{y}}{\partial \mathbf{x}}  = \begin{bmatrix}
\frac{\partial}{\partial x_1} ( f_{1}(\mathbf{x}) \bigcirc g_{1}(\mathbf{x}) ) & \frac{\partial}{\partial x_2} ( f_{1}(\mathbf{x}) \bigcirc g_{1}(\mathbf{x}) ) & \ldots & \frac{\partial}{\partial x_n} ( f_{1}(\mathbf{x}) \bigcirc g_{1}(\mathbf{x}) )\\\\
\frac{\partial}{\partial x_1} ( f_{2}(\mathbf{x}) \bigcirc g_{2}(\mathbf{x}) ) & \frac{\partial}{\partial x_2} ( f_{2}(\mathbf{x}) \bigcirc g_{2}(\mathbf{x}) ) & \ldots & \frac{\partial}{\partial x_n} ( f_{2}(\mathbf{x}) \bigcirc g_{2}(\mathbf{x}) )\\\\
& \ldots\\\\
\frac{\partial}{\partial x_1} ( f_{n}(\mathbf{x}) \bigcirc g_{n}(\mathbf{x}) ) & \frac{\partial}{\partial x_2} ( f_{n}(\mathbf{x}) \bigcirc g_{n}(\mathbf{x}) ) & \ldots & \frac{\partial}{\partial x_n} ( f_{n}(\mathbf{x}) \bigcirc g_{n}(\mathbf{x}) )\\
\end{bmatrix}
$

That's quite a furball, but the Jacobian is very commonly a diagonal matrix, a matrix that is zero everywhere but the diagonal. 

If $f_i(\mathbf{x})$ and $g_i(\mathbf{x})$ are purely functions of at most $x_i$, and no other elements of vector $\mathbf{x}$, we can reduce $f_i(\mathbf{x}) \bigcirc g_i(\mathbf{x})$ to $f_i(x_i) \bigcirc g_i(x_i)$. This is technically an abuse of our notation because $f_i$ and $g_i$ are functions of vectors not individual elements. We should really write something like $\hat f_{i}(x_i) = f_{i}(\mathbf{x})$, but that would muddy the equations further (and programmers are comfortable overloading functions) so we'll proceed with the notation anyway.

With the constraint that $f_i(\mathbf{x})$ and $g_i(\mathbf{x})$ can only access $x_i$, the elements along the diagonal of the Jacobian are $\frac{\partial}{\partial x_i} ( f_i(x_i) \bigcirc g_i(x_i) )$:

$
\frac{\partial \mathbf{y}}{\partial \mathbf{x}}  = \begin{bmatrix}
\frac{\partial}{\partial x_1} ( f_{1}(x_1) \bigcirc g_{1}(x_1) )\\
& \frac{\partial}{\partial x_2} (f_{2}(x_2) \bigcirc g_{2}(x_2) ) & &\text{\huge0}\\
& & \ldots \\
\text{\huge0}& & & \frac{\partial}{\partial x_n} (f_{n}(x_n) \bigcirc g_{n}(x_n) )
\end{bmatrix}
$

Or more succinctly,

$\frac{\partial \mathbf{y}}{\partial \mathbf{x}} = diag \left( \frac{\partial}{\partial x_1}(f_{1}(x_1) \bigcirc g_{1}(x_1)),~ \frac{\partial}{\partial x_2}(f_{2}(x_2) \bigcirc g_{2}(x_2)),~ \ldots,~ \frac{\partial}{\partial x_n}(f_{n}(x_n) \bigcirc g_{n}(x_n)) \right)$. 

We know that all off-diagonal entries are 0 because $\frac{\partial}{\partial x_j} ( f_i(x_i) \bigcirc g_i(x_i) ) = 0$ for $j \neq i$. When $j \neq i$, $f_i(x_i) \bigcirc g_i(x_i)$ looks like a constant to the partial derivative operator $\frac{\partial}{\partial x_j}$, and the derivative of any constant is 0.

Because we do lots of simple vector arithmetic, the general function $\mathbf{f(x)}$ is often just the vector $\mathbf{x}$.  Any time the general function is a vector, we know that $f_i(\mathbf{x})$ reduces to $f_i(x_i) = x_i$. For example, vector addition $\mathbf{w + x}$ fits our general case as $\mathbf{f(w)} + \mathbf{g(x)}$ with scalar equations $y_i = f_i(\mathbf{w}) + g_i(\mathbf{x})$. But, that reduces to just $y_i = f_i(w_i) + g_i(x_i) = w_i + x_i$ and so we get partial derivative:

$\frac{\partial}{\partial x_i} ( f_{i}(w_i) + g_{i}(x_i) ) = \frac{\partial}{\partial x_i}(w_i + x_i) = 1 + 0 = 1$

That gives us $\frac{\partial (\mathbf{w+x})}{\partial \mathbf{w}} = I$, the identity matrix, because every element along the diagonal is 1.  Similarly, $\frac{\partial (\mathbf{w+x})}{\partial \mathbf{x}} = I$.

Given the simplicity of this special case, $f_i(\mathbf{x})$ reducing to $f_i(x_i)$, we can easily write out the Jacobian's for the common element-wise binary operations on vectors:

$
\begin{array}{lllll}
\text{Op} & \text{Partial with respect to } \mathbf{w} & \text{Partial with respect to }\mathbf{x}\\
\hline\\

+ & \frac{\partial (\mathbf{w+x})}{\partial \mathbf{w}} = diag(\ldots \frac{\partial (w_i + x_i)}{\partial w_i} \ldots) = diag(\vec{1}) = I & \frac{\partial (\mathbf{w+x})}{\partial \mathbf{x}} =  I\\\\

- & \frac{\partial (\mathbf{w-x})}{\partial \mathbf{w}}  =  diag(\ldots\frac{\partial (w_i - x_i)}{\partial w_i}\ldots) =  diag(\vec{1})  =  I & \frac{\partial (\mathbf{w-x})}{\partial \mathbf{x}}  =  diag(\ldots\frac{\partial (w_i - x_i)}{\partial x_i}\ldots)  =  diag(-\vec{1})  =  -I \\\\

\otimes & \frac{\partial (\mathbf{w \otimes x})}{\partial \mathbf{w}}  =  diag(\ldots\frac{\partial (w_i \times x_i)}{\partial w_i} \ldots)  =  diag(\mathbf{x}) & \frac{\partial (\mathbf{w \otimes x})}{\partial \mathbf{x}}  =  diag(\mathbf{w})\\\\

\oslash & \frac{\partial (\mathbf{w \oslash x})}{\partial \mathbf{w}}  =  diag(\ldots\frac{\partial (w_i / x_i)}{\partial w_i}\ldots)  =  diag(\ldots \frac{1}{x_i} \ldots) & \frac{\partial (\mathbf{w \oslash x})}{\partial \mathbf{x}}  =  diag(\ldots \frac{-w_i}{x_i^2} \ldots)\\

\end{array}
$

\subsection{Derivatives involving scalar expansion}

When we add or multiply scalars to vectors, we are implicitly expanding the scalar to a vector and then performing an element-wise operation. For example, vector-scalar addition 
$\mathbf{y} = \mathbf{x} + c$
is really
$\mathbf{y} = \mathbf{f(x)} + \mathbf{g}(c)$ where $\mathbf{f(x)} = \mathbf{x}$ and $\mathbf{g}(c) = \vec{1} \times c$.
Similarly, vector-scalar multiplication
$\mathbf{y} = \mathbf{x} \times c$
is really
$\mathbf{y} = \mathbf{f(x)} \otimes \mathbf{g}(c)$.

The partial derivative of vector-scalar addition and multiplication with respect to vector $\mathbf{x}$  fall out from our element-wise rule, $\frac{\partial \mathbf{y}}{\partial \mathbf{x}} = diag \left( \ldots \frac{\partial}{\partial x_i} ( f_i(x_i) \bigcirc g_i(c) ) \ldots \right)$, since we've identified $f_i(x_i) = x_i$ and $g_i(c) = c$. Note that functions $\mathbf{f(x)} = \mathbf{x}$ and $\mathbf{g}(c) = \vec{1} \times c$ clearly satisfy our constraint for the special case of diagonal Jacobian (that $f_i(\mathbf{x})$ refer at most to $x_i$ and $g_i(c)$ refers to the $i^{th}$ value of the $\vec{1}$ vector). 

Using the usual rules for scalar partial derivatives, we arrive at the following diagonal elements of the Jacobian for vector-scalar addition:
 
$\frac{\partial}{\partial x_i} ( f_i(x_i) + g_i(c) ) = \frac{\partial (x_i + c)}{\partial x_i} = \frac{\partial x_i}{\partial x_i} + \frac{\partial c}{\partial x_i} = 1 + 0 = 1$

So, $\frac{\partial}{\partial \mathbf{x}} ( \mathbf{x} + c ) = diag(\vec{1}) = I$.

Computing the partial derivative with respect to the single-valued parameter $c$, however, results in a vertical vector not a diagonal matrix whose elements are:
 
$\frac{\partial}{\partial c} ( f_i(x_i) + g_i(c) ) = \frac{\partial (x_i + c)}{\partial c} = \frac{\partial x_i}{\partial c} + \frac{\partial c}{\partial c} = 0 + 1 = 1$

Therefore, $\frac{\partial}{\partial c} ( \mathbf{x} + c ) = \vec{1}$.

The diagonal elements of the Jacobian for vector-scalar multiplication involve the product rule for scalar derivatives:

$\frac{\partial}{\partial x_i} ( f_i(x_i) \otimes g_i(c) ) = x_i \times \frac{\partial c}{\partial x_i} + c \times \frac{\partial x_i}{\partial x_i} = 0 + c = c$

So, $\frac{\partial}{\partial \mathbf{x}} ( \mathbf{x} \times c ) = diag(\vec{1} \times c) = I \times c$. 

The partial derivative with respect to single-valued parameter $c$ is a vertical vector whose elements are:

$\frac{\partial}{\partial c} ( f_i(x_i) \otimes g_i(c) ) = x_i \times \frac{\partial c}{\partial c} + c \times \frac{\partial x_i}{\partial c} = x_i + 0 = x_i$

This gives us $\frac{\partial}{\partial c} ( \mathbf{x} \times c ) = \mathbf{x}$.

\subsection{Vector sum}

Summing the elements of a vector is an important operation in and of itself in applications based upon linear algebra, but we can also use it as a way to simplify computing the derivative of vector dot product and other operations that reduce vectors to scalars.

Let $y = sum( \mathbf{f}(\mathbf{x})) = \Sigma_{i=1}^n f_i(\mathbf{x})$ where we have to be careful to leave the parameter as a vector $\mathbf{x}$ because each function $f_i$ could use all values in the vector not just $x_i$. The sum is over the {\bf results} of the function and not the parameter. The gradient ($1 \times n$ Jacobian) of vector summation is:

$
\begin{array}{lcl}
\frac{\partial y}{\partial \mathbf{x}} & = & \begin{bmatrix} \frac{\partial y}{\partial x_1}, \frac{\partial y}{\partial x_2}, \ldots, \frac{\partial y}{\partial x_n} \end{bmatrix}\\\\
 & = & \begin{bmatrix} \frac{\partial}{\partial x_1} \Sigma_i f_i(\mathbf{x}),~ \frac{\partial}{\partial x_2} \Sigma_i f_i(\mathbf{x}),~ \ldots,~ \frac{\partial}{\partial x_n} \Sigma_i  f_i(\mathbf{x}) \end{bmatrix} \\\\
 & = & \begin{bmatrix} \Sigma_i \frac{\partial f_i(\mathbf{x})}{\partial x_1},~ \Sigma_i \frac{\partial f_i(\mathbf{x})}{\partial x_2},~ \ldots,~ \Sigma_i \frac{\partial f_i(\mathbf{x})}{\partial x_n}  \end{bmatrix}(\text{move derivative inside }\Sigma)\\\\
\end{array}
$

The summation inside the gradient elements can be tricky so make sure to keep your notation consistent. 

Let's look at the gradient of the simple $y = sum(\mathbf{x})$. The function inside the summation is just $f_i(\mathbf{x}) = x_i$ and the gradient is then:

$\nabla y = \begin{bmatrix} \Sigma_i \frac{\partial f_i(\mathbf{x})}{\partial x_1},~ \Sigma_i \frac{\partial f_i(\mathbf{x})}{\partial x_2},~ \ldots,~ \Sigma_i \frac{\partial f_i(\mathbf{x})}{\partial x_n}  \end{bmatrix} = \begin{bmatrix} \Sigma_i \frac{\partial x_i}{\partial x_1},~ \Sigma_i \frac{\partial x_i}{\partial x_2},~ \ldots,~ \Sigma_i \frac{\partial x_i}{\partial x_n}  \end{bmatrix}$

Because $\frac{\partial}{\partial x_j} x_i = 0$ for $j \neq i$, we can simplify to:

$\nabla y = \begin{bmatrix} \frac{\partial x_1}{\partial x_1},~ \frac{\partial x_2}{\partial x_2},~ \ldots,~ \frac{\partial x_n}{\partial x_n}  \end{bmatrix} = \begin{bmatrix}1, 1, \ldots, 1\end{bmatrix} = \vec{1}^T$

Notice that the result is a horizontal vector full of 1s, not a vertical vector, and so the gradient is $\vec{1}^T$. It's very important to keep the shape of all of your vectors and matrices in order otherwise it's impossible to compute the derivatives of complex functions.

As another example, let's sum a vector-scalar multiplication.  If $y = sum(\mathbf{x} \times c)$ then $f_i(\mathbf{x},c) = x_i \times c$. The gradient is:

$
\begin{array}{lcl}
\frac{\partial y}{\partial \mathbf{x}} & = & \begin{bmatrix} \Sigma_i \frac{\partial}{\partial x_1} x_i c,~ \Sigma_i \frac{\partial }{\partial x_2} x_i c,~ \ldots,~ \Sigma_i \frac{\partial}{\partial x_n} x_i c  \end{bmatrix}\\\\
 & = & \begin{bmatrix} \frac{\partial}{\partial x_1} x_1 c,~ \frac{\partial }{\partial x_2} x_2 c,~ \ldots,~ \frac{\partial}{\partial x_n} x_n c  \end{bmatrix}\\\\
 & = & \begin{bmatrix} c, c, \ldots, c \end{bmatrix}\\\\
\end{array}
$

The derivative with respect to scalar variable $c$ is $1 \times 1$:

$
\begin{array}{lcl}
\frac{\partial y}{\partial c} & = & \frac{\partial}{\partial c} \Sigma_{i=1}^n (x_i+c)\\\\
& = & \Sigma_i \frac{\partial}{\partial c} (x_i+c)\\\\
& = & \Sigma_i (0 + 1)\\\\
& = & n
\end{array}
$

\subsection{Vector chain rule}

We can't compute partial derivatives of very complicated functions using just the basic matrix calculus rules we've seen so far. We need to be able to combine them using the so-called {\em chain rule}.  The chain rule is a divide and conquer strategy (like Quicksort) that breaks complicated expressions into subexpressions that are easier to compute derivatives for.  The chain rule comes into play when we need the derivative of an expression with nested subexpressions, such as $\frac{d}{dx} sin(x^2)$.  

In general, the chain rule is defined in terms of expressions that can be written as $y = f(g(x))$. (You will also see the chain rule defined using function composition $(f \circ g)(x)$ which is the same thing.)  Some sources write the derivative using shorthand notation $f'(g(x))g'(x)$, but that hides the fact that we are performing a variable substitution: $u = g(x)$. It's better to define the chain rule explicitly via the following so we never take the derivative with respect to the wrong variable.

$\frac{dy}{dx} = \frac{dy}{du} \times \frac{du}{dx}$.

In this case, we split $sin(x^2)$ into two functions:

$u = g(x) = x^2$\\
$y = f(u) = sin(u)$

Putting the functions $f$ and $g$ and their parameters in there makes it clear which variables we must take the derivative with respect to. The order of these subexpressions does not affect the answer, but we recommend working in the reverse order of operations dictated by the nesting (innermost to outermost). That way, expressions and derivatives are always functions of previously-computed elements. Here are the derivatives of the simplified functions:

$\frac{du}{dx} = \frac{d}{dx} x^2 = 2x$\\
$\frac{dy}{du} = \frac{d}{du} sin(u) = cos(u)$

Now, we just have to combine the derivatives using the chain rule, which means multiplying the two derivatives together:

$\frac{dy}{dx} = \frac{dy}{du} \times \frac{du}{dx} = cos(u) \times 2x$

then substituting the intermediate variable back in

$\frac{dy}{dx} = cos(x^2) \times 2x = 2xcos(x^2)$

You can think of the chain rule in terms of units canceling. If we let $y$ be gallons of gas, $x$ be the gallons in a gas tank, and $u$ as miles we can interpret $\frac{dy}{dx}$ as $\frac{miles}{tank} = \frac{miles}{gallon} \times \frac{gallon}{tank}$. The $gallon$ denominator and numerator cancel. This is a convenient way to remember the chain rule but the analogy only goes so far; don't treat $dy$ and $dx$ has separate variables since they are two components in the name of a single operator.

When there is only one variable substitution, it's straightforward to directly write the derivative down without explicitly creating and substituting a temporary variable. With deeply nested expressions, however, it's easy to get lost and it's better to carefully track all of the intermediate variables.

It might help to think about deploying the chain rule the way a compiler implementor thinks about generating code for complicated expressions.  The compiler must unravel function call nesting like $f_1(f_2(f_3(f_4(x))))$ into a sequence of calls. The result of calling function $f_i$ is saved to a temporary variable called a register, which is then passed as a parameter to $f_{i-1}$.  Consider a nested equation like $y = f(x) = ln(sin(x^3)^2)$ that breaks down into subexpressions assigned to  intermediate (temporary) variables:

$r_1 = f_1(x) = x^3$\\
$r_2 = f_2(r_1) = sin(r_1)$\\
$r_3 = f_3(r_2) = r_2^2$\\
$r_4 = f_4(r_3) = ln(r_3)$ ~~~~~~($y = r_4$)

To compute the scalar derivative $\frac{dy}{dx}$, we can treat each of those subexpressions in isolation using just the basic scalar derivative rules.

$
\begin{array}{lllll}
\vspace{1mm}
\frac{d}{r_x} r_1 & = & \frac{d}{x} x^3 & = & 3x^2\\
\vspace{1mm}
\frac{d}{r_1} r_2 & = & \frac{d}{r_1} sin(r_1) & = & cos(r_1) \\
\frac{d}{r_2} r_3 & = & \frac{d}{r_2} r_2^2 & =& 2r_2\\
\frac{d}{r_3} r_4 & = & \frac{d}{r_3} ln(r_3) & =& \frac{1}{r_3}\\
\end{array}
$

Using the chain rule for 4 intermediate variables,

$\frac{dy}{dx} = \frac{dy}{r_4} \times \frac{d r_4}{r_3} \times \frac{dr_3}{d r_2} \times \frac{dr_2}{dr_1} \times \frac{dr_1}{dx}$

we get

$\frac{dy}{dx} = 1 \times \frac{1}{r_3} \times 2r_2 \times cos(r_1) \times 3x^2 = \frac{6r_2x^2cos(r_1)}{r_3}$

Substituting the intermediate variables back in, gives us:

$\frac{dy}{dx} = \frac{6sin(r_1)x^2cos(x^3)}{r_2^2} = \frac{6sin(x^3)x^2cos(x^3)}{sin(r_1)^2} = \frac{6sin(x^3)x^2cos(x^3)}{sin(x^3)^2} = \frac{6x^2cos(x^3)}{sin(x^3)}$

Ok, so much for the scalar chain rule. It turns out that the chain rule also works for computing partial derivatives of vector functions with respect to vectors.  Previously, we manually computed the derivative with respect to $\mathbf{x}$ of $y = sum(\mathbf{x} \times c)$ as $[c, c, \ldots, c]$, but we can also use the chain rule. Let vector $\mathbf{u} = \mathbf{x} \times c$ then $y = sum(\mathbf{u})$. The vector chain rule tells us that $\frac{dy}{d\mathbf{x}} = \frac{dy}{d\mathbf{u}} \times \frac{d\mathbf{u}}{d\mathbf{x}}$, which means 

$\frac{dy}{d\mathbf{x}} = \vec{1}^T \times \frac{d}{d\mathbf{x}} (\mathbf{x} \times c) = \vec{1}^T \times diag(\vec{1}c) = \vec{1}^T \times I \times c = \vec{1}^T \times c = [c, c, \ldots, c]$

\subsection{Derivatives of vector dot product}

Now that we've got the chain rule in mind and we can compute the Jacobians for both element-wise binary operations and vector summation, we can define the gradient of the important vector dot product $y = \mathbf{f(x)} \cdot \mathbf{g(x)}$. (Note we use $y$ not $\mathbf{y}$ as the result is a scalar not a vector.) You will find the derivative of dot product defined as $f(\mathbf{x}) \cdot g'(\mathbf{x}) + f'(\mathbf{x}) \cdot g(\mathbf{x})$, but we'll use the chain rule instead to avoid having to memorize yet another rule.

Notice that dot product $\mathbf{w} \cdot \mathbf{x}$ is just the summation of the element-wise multiplication of the elements: $\Sigma_i^n (w_i x_i) = sum(\mathbf{w} \otimes \mathbf{x})$. (You might also find it useful to remember the linear algebra notation $\mathbf{w} \cdot \mathbf{x} = \mathbf{w}^{T} \times \mathbf{x}$.)  To use the chain rule, we perform a quick substitution, $\mathbf{u} = \mathbf{w} \otimes \mathbf{x}$ and $y = sum(\mathbf{u})$, and multiply their partial derivatives:

$\frac{d \mathbf{u}}{d\mathbf{x}} = \frac{d}{d\mathbf{x}} (\mathbf{w} \otimes \mathbf{x}) = diag(\mathbf{w})$\\
$\frac{dy}{d\mathbf{u}} = \frac{d}{d\mathbf{u}} sum(\mathbf{u}) = \vec{1}^T$

$\frac{dy}{d\mathbf{x}} = \frac{dy}{d\mathbf{u}} \times \frac{d\mathbf{u}}{d\mathbf{x}} = \vec{1}^T \times diag(\mathbf{w}) = \mathbf{w}^T$

Similarly, $\frac{dy}{d\mathbf{w}} = \mathbf{x}^T$. Note: the gradient is a horizontal vector (1 output)!

To check our results, we can also grind the dot product down into a pure scalar function:

$y = \mathbf{w} \cdot \mathbf{x} = \Sigma_i^n (w_i x_i)$

$\frac{\partial y}{\partial w_j} = \frac{\partial}{\partial w_j} \Sigma_i (w_i x_i) = \Sigma_i \frac{\partial}{\partial w_j} (w_i x_i) = \frac{\partial}{\partial w_j} (w_j x_j) = x_j$

Then:

$\frac{\partial y}{\partial \mathbf{w}} = [ x_1, \ldots, x_n ] = \mathbf{x}^T$

\section{Acknowledgements}

We thank [Yannet Interian](https://www.usfca.edu/faculty/yannet-interian) (Faculty in MS data science program at University of San Francisco) and [David Uminsky](http://www.cs.usfca.edu/~duminsky/) (Faculty/director of MS data science) for their help with the notation presented here..

\end{document}
